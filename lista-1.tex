% Options for packages loaded elsewhere
\PassOptionsToPackage{unicode}{hyperref}
\PassOptionsToPackage{hyphens}{url}
%
\documentclass[
]{article}
\usepackage{amsmath,amssymb}
\usepackage{lmodern}
\usepackage{ifxetex,ifluatex}
\ifnum 0\ifxetex 1\fi\ifluatex 1\fi=0 % if pdftex
  \usepackage[T1]{fontenc}
  \usepackage[utf8]{inputenc}
  \usepackage{textcomp} % provide euro and other symbols
\else % if luatex or xetex
  \usepackage{unicode-math}
  \defaultfontfeatures{Scale=MatchLowercase}
  \defaultfontfeatures[\rmfamily]{Ligatures=TeX,Scale=1}
\fi
% Use upquote if available, for straight quotes in verbatim environments
\IfFileExists{upquote.sty}{\usepackage{upquote}}{}
\IfFileExists{microtype.sty}{% use microtype if available
  \usepackage[]{microtype}
  \UseMicrotypeSet[protrusion]{basicmath} % disable protrusion for tt fonts
}{}
\makeatletter
\@ifundefined{KOMAClassName}{% if non-KOMA class
  \IfFileExists{parskip.sty}{%
    \usepackage{parskip}
  }{% else
    \setlength{\parindent}{0pt}
    \setlength{\parskip}{6pt plus 2pt minus 1pt}}
}{% if KOMA class
  \KOMAoptions{parskip=half}}
\makeatother
\usepackage{xcolor}
\IfFileExists{xurl.sty}{\usepackage{xurl}}{} % add URL line breaks if available
\IfFileExists{bookmark.sty}{\usepackage{bookmark}}{\usepackage{hyperref}}
\hypersetup{
  pdftitle={Lista1},
  pdfauthor={Silvaneo Viera dos Santos Junior},
  hidelinks,
  pdfcreator={LaTeX via pandoc}}
\urlstyle{same} % disable monospaced font for URLs
\usepackage[margin=1in]{geometry}
\usepackage{color}
\usepackage{fancyvrb}
\newcommand{\VerbBar}{|}
\newcommand{\VERB}{\Verb[commandchars=\\\{\}]}
\DefineVerbatimEnvironment{Highlighting}{Verbatim}{commandchars=\\\{\}}
% Add ',fontsize=\small' for more characters per line
\usepackage{framed}
\definecolor{shadecolor}{RGB}{248,248,248}
\newenvironment{Shaded}{\begin{snugshade}}{\end{snugshade}}
\newcommand{\AlertTok}[1]{\textcolor[rgb]{0.94,0.16,0.16}{#1}}
\newcommand{\AnnotationTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\AttributeTok}[1]{\textcolor[rgb]{0.77,0.63,0.00}{#1}}
\newcommand{\BaseNTok}[1]{\textcolor[rgb]{0.00,0.00,0.81}{#1}}
\newcommand{\BuiltInTok}[1]{#1}
\newcommand{\CharTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\CommentTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textit{#1}}}
\newcommand{\CommentVarTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\ConstantTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\ControlFlowTok}[1]{\textcolor[rgb]{0.13,0.29,0.53}{\textbf{#1}}}
\newcommand{\DataTypeTok}[1]{\textcolor[rgb]{0.13,0.29,0.53}{#1}}
\newcommand{\DecValTok}[1]{\textcolor[rgb]{0.00,0.00,0.81}{#1}}
\newcommand{\DocumentationTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\ErrorTok}[1]{\textcolor[rgb]{0.64,0.00,0.00}{\textbf{#1}}}
\newcommand{\ExtensionTok}[1]{#1}
\newcommand{\FloatTok}[1]{\textcolor[rgb]{0.00,0.00,0.81}{#1}}
\newcommand{\FunctionTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\ImportTok}[1]{#1}
\newcommand{\InformationTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\KeywordTok}[1]{\textcolor[rgb]{0.13,0.29,0.53}{\textbf{#1}}}
\newcommand{\NormalTok}[1]{#1}
\newcommand{\OperatorTok}[1]{\textcolor[rgb]{0.81,0.36,0.00}{\textbf{#1}}}
\newcommand{\OtherTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{#1}}
\newcommand{\PreprocessorTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textit{#1}}}
\newcommand{\RegionMarkerTok}[1]{#1}
\newcommand{\SpecialCharTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\SpecialStringTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\StringTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\VariableTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\VerbatimStringTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\WarningTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\usepackage{graphicx}
\makeatletter
\def\maxwidth{\ifdim\Gin@nat@width>\linewidth\linewidth\else\Gin@nat@width\fi}
\def\maxheight{\ifdim\Gin@nat@height>\textheight\textheight\else\Gin@nat@height\fi}
\makeatother
% Scale images if necessary, so that they will not overflow the page
% margins by default, and it is still possible to overwrite the defaults
% using explicit options in \includegraphics[width, height, ...]{}
\setkeys{Gin}{width=\maxwidth,height=\maxheight,keepaspectratio}
% Set default figure placement to htbp
\makeatletter
\def\fps@figure{htbp}
\makeatother
\setlength{\emergencystretch}{3em} % prevent overfull lines
\providecommand{\tightlist}{%
  \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}
\setcounter{secnumdepth}{-\maxdimen} % remove section numbering
\usepackage{cancel}
\usepackage{dsfont}
\ifluatex
  \usepackage{selnolig}  % disable illegal ligatures
\fi

\title{Lista1}
\author{Silvaneo Viera dos Santos Junior}
\date{}

\begin{document}
\maketitle

\hypertarget{questuxe3o-1}{%
\subsection{Questão 1}\label{questuxe3o-1}}

\hypertarget{a}{%
\subsubsection{a)}\label{a}}

Primeiramente, observe que:

\[
\lim_{\Delta \xi \rightarrow0^-}\frac{h_\xi(\xi+\Delta \xi)-h_\xi(\xi)}{\Delta \xi}=\lim_{\Delta \xi \rightarrow0^-}\frac{\max(0,\cancel{\xi}+\Delta \xi-\cancel{\xi})^3-\max(0,\cancel{\xi}-\cancel{\xi})^3}{\Delta \xi}=\lim_{\Delta \xi \rightarrow0^-}\frac{\max(0,\Delta \xi)^3}{\Delta \xi}=0
\]

Sendo que a última igualdade vale, pois estamos tomando o limite à
esquerda, i.e., \(\Delta \xi<0\).

Vejamos agora que:

\[
\lim_{\Delta \xi \rightarrow0^+}\frac{h_\xi(\xi+\Delta \xi)-h_\xi(\xi)}{\Delta \xi}=\lim_{\Delta \xi \rightarrow0^+}\frac{\max(0,\Delta \xi)^3}{\Delta \xi}=\lim_{\Delta \xi \rightarrow0^+}\frac{\Delta \xi^3}{\Delta \xi}=\lim_{\Delta \xi \rightarrow0^+}\Delta \xi^2=0
\]

Logo, para todo nó \(\xi\), o limite
\(\lim_{\Delta \xi \rightarrow0^+}\frac{h_\xi(\xi+\Delta \xi)-h_\xi(\xi)}{\Delta \xi}\)
existe e é igual a \(0\), ou seja, \(h\) é diferenciável em todos os nós
\(\xi\).

\hypertarget{b}{%
\subsubsection{b)}\label{b}}

É fácil perceber que:

\[
h'_\xi(x)=
\begin{cases}
\begin{aligned}
3(x-\xi)^2,&\text{ se }x>\xi\\
0,&\text{ caso contrário.}
\end{aligned}
\end{cases}
\]

Observe que:

\[
\lim_{\Delta \xi \rightarrow0^-}\frac{h'_\xi(\xi+\Delta \xi)-h'_\xi(\xi)}{\Delta \xi}=\lim_{\Delta \xi \rightarrow0^-}\frac{0}{\Delta \xi}=0
\]

Sendo que, novamente, como estamos tomando o limite à esquerda, vale que
\(\xi+\Delta \xi<\xi\), logo \(h'_\xi(\xi+\Delta \xi)=0\).

Ademais, podemos verificar que:

\[
\lim_{\Delta \xi \rightarrow0^+}\frac{h'_\xi(\xi+\Delta \xi)-h'_\xi(\xi)}{\Delta \xi}=\lim_{\Delta \xi \rightarrow0^+}\frac{3(\xi+\Delta\xi-\xi)^2-0}{\Delta \xi}=\lim_{\Delta \xi \rightarrow0^+}\frac{3\Delta\xi^2}{\Delta \xi}=\lim_{\Delta \xi \rightarrow0^+}3\Delta\xi=0
\]

Concluímos assim que o limite
\(\lim_{\Delta \xi \rightarrow0}\frac{h'_\xi(\xi+\Delta \xi)-h'_\xi(\xi)}{\Delta \xi}\)
existe e é igual a \(0\), ou seja, \(h\) é duplamente diferenciável em
qualquer nó \(\xi\).

\hypertarget{c}{%
\subsubsection{c)}\label{c}}

É fácil ver que

\[
h''_\xi(x)=
\begin{cases}
\begin{aligned}
6(x-\xi),&\text{ se }x>\xi\\
0,&\text{ caso contrário.}
\end{aligned}
\end{cases}
\]

Trivialmente, \(6(x-\xi)\) é contínua em \(\mathbb{R}\), assim como a
função identicamente nula, logo, resta apenas mostrar que \(h''\) é
contínua em \(0\) (para os outros pontos de \(\mathbb{R}\), a
continuidade é ``herdada'' das funções identicamente nula e
\(6(x-\xi)\)).

Veja que:

\[
\lim_{\Delta\xi \rightarrow 0} 6(\cancel{\xi}+\Delta\xi-\cancel{\xi})=\lim_{\Delta\xi \rightarrow 0} 6\Delta\xi=0
\]

Daí:

\[
\lim_{\Delta \xi \rightarrow 0^-} h''_\xi(\xi+\Delta\xi)=\lim_{\Delta \xi \rightarrow 0^+} h''_\xi(\xi+\Delta\xi)=\lim_{x \rightarrow 0^+} 6\Delta\xi=0=h''_\xi(\xi)
\]

Isto é, \(h''\) é contínua em todo nó \(\xi\).

\hypertarget{d}{%
\subsubsection{d)}\label{d}}

Veja que \(\beta_0+\beta_1x+\beta_2x^2+\beta_3x^3\) é um polinômio,
portanto, é infinitamente diferenciável, ademais, sabemos que o conjunto
das funções de classe \(C^2\) é fechado para a operação de soma finita,
daí, como \(K\) é um valor pré-fixado, vale que
\(\sum_{k=1}^K\beta_{k+3}h_{\xi_k}(x)\) é de classe \(C^2\), com isto
podemos concluir que
\(s(x)=\beta_0+\beta_1x+\beta_2x^2+\beta_3x^3+\sum_{k=1}^K\beta_{k+3}h_{\xi_k}(x)\)
é de classe \(C^2\).

\hypertarget{questuxe3o-2}{%
\subsection{Questão 2}\label{questuxe3o-2}}

\hypertarget{a-1}{%
\subsubsection{a)}\label{a-1}}

\[
\begin{aligned}
&y \sim \mathcal{N}(\mu,\sigma^2)\\
&\mu = g(X\beta)\Rightarrow \text{ função de ligação }  g(x)=x\\
&\text{A }i\text{-ésima linha de }\boldsymbol{X}\text{ é igual a }\boldsymbol{x_i}=\left[1,\ln x_i\right]
\end{aligned}
\]

\hypertarget{b-1}{%
\subsubsection{b)}\label{b-1}}

\[
\begin{aligned}
&y \sim \textbf{Bin}(n,p)\\
&p = g(X\beta)\Rightarrow \text{ função de ligação }  g(x)=\frac{1}{1+e^{-x}}\\
&\text{A }i\text{-ésima linha de }\boldsymbol{X}\text{ é igual a }\boldsymbol{x_i}=
\begin{bmatrix}
1\\
x_i\\
x_i^2\\
x_i^3\\
\max\{0,(x_i-0.2)\}^3\\
\max\{0,(x_i-1.3)\}^3\\
\max\{0,(x_i-2)\}^3\\
\max\{0,(x_i-3.2)\}^3
\end{bmatrix}^t
\end{aligned}
\]

\hypertarget{c-1}{%
\subsubsection{c)}\label{c-1}}

Primeiro, vamos definir algumas funções auxiliares:

\[
\begin{aligned}
\sigma(x)=&(1+e^{-x})^{-1}\\
\text{softmax}(\vec{x})=&
\frac{\sigma(\vec{x})}{||\sigma(\vec{x})||_1}
\end{aligned}
\]

Onde consideramos, para \(f:\mathbb{R}\rightarrow \mathbb{R}\), que:

\[
f(\vec{x})=
\begin{bmatrix}
f(x_1)\\
\vdots\\
f(x_n)\\
\end{bmatrix}
\]

Ademais, definimos a norma \(||\vec{x}||_1\) como:

\[||\vec{x}||_1 = \sum_{i=1}^n|x_i|\]

No caso particular onde \(\vec{x}\) possui todas as entradas positivas,
temos que \(||\vec{x}||_1 = \sum_{i=1}^nx_i\).

Vale destacar que:

\[||\text{softmax}(\vec{x})||_1 =1\]

Seguiremos agora com a descrição do modelo:

\[
\begin{aligned}
&y \sim \textbf{Categorica}(\vec{p})\\
&\vec{p} = g(X\beta)\Rightarrow \text{ função de ligação }  g(\vec{x})=\text{softmax}(\vec{x})\\
&\text{A }i\text{-ésima linha de }\boldsymbol{X}\text{ é igual a }\boldsymbol{x_i}=
\begin{bmatrix}
1\\
x_i\\
x_i^2\\
x_i^3\\
\max\{0,(x_i-0)\}^3\\
\max\{0,(x_i-1)\}^3\\
\max\{0,(x_i-2)\}^3
\end{bmatrix}^t
\end{aligned}
\]

\hypertarget{questuxe3o-3}{%
\subsection{Questão 3}\label{questuxe3o-3}}

Nas questões a seguir (\(3\) e \(4\)), alternamos entre o uso das
linguagens \emph{R} (para gráficos) e \emph{Python} (para cálculos um
pouco mais intesivos), sendo que o pacote \emph{reticulate} foi
utilizado para intermediá-las, desta forma, a variável \(py\) no \(R\)
armazena as variáveis do \emph{Python} (ou seja, o comando
\emph{py\$exemplo} acessa a variável \emph{exemplo} do \emph{python} no
\emph{R}) e, de forma análoga, a variável \emph{r} no \emph{Python}
armazena as variáveis do \emph{R} (ou seja, o comando \emph{r.exemplo}
acessa a variável \emph{exemplo} do \emph{R} no \emph{Python}). Para
evitar confusões entre as linguagens, colocamos um indicativo de qual a
linguagem usada no início de cada bloco de código.

Vale destacar também que usamos a versão \(3.8.10\) do \emph{Python} e
os pacotes \emph{Numpy} e \emph{Tensorflow}, ademais usamos a versão
\(4.1.0\) do \emph{R} com os pacotes \emph{MASS}, \emph{ggplot2},
\emph{latex2exp}, \emph{tidyr} e \emph{reticulate}.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{knitr}\SpecialCharTok{::}\NormalTok{opts\_chunk}\SpecialCharTok{$}\FunctionTok{set}\NormalTok{(}\AttributeTok{echo =} \ConstantTok{TRUE}\NormalTok{)}

\CommentTok{\# Este código deve ser usado no RStudio integrado a algum Kernel do Python.}
\CommentTok{\# A versão do RStudio usada é 1.4.1717.}
\CommentTok{\# A versão do Python usada é 3.8.10.}
\CommentTok{\# A versão do knitr usada é 1.33.}
\FunctionTok{library}\NormalTok{(MASS)       }\CommentTok{\# Versão 7.3{-}54}
\FunctionTok{library}\NormalTok{(ggplot2)    }\CommentTok{\# Versão 3.3.5}
\FunctionTok{library}\NormalTok{(latex2exp)  }\CommentTok{\# Versão 0.5.0}
\FunctionTok{library}\NormalTok{(tidyr)      }\CommentTok{\# Versão 1.1.3}
\FunctionTok{library}\NormalTok{(reticulate) }\CommentTok{\# Versão 1.20}

\FunctionTok{sessionInfo}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## R version 4.1.0 (2021-05-18)
## Platform: x86_64-w64-mingw32/x64 (64-bit)
## Running under: Windows 10 x64 (build 19043)
## 
## Matrix products: default
## 
## locale:
## [1] LC_COLLATE=English_United States.1252 
## [2] LC_CTYPE=English_United States.1252   
## [3] LC_MONETARY=English_United States.1252
## [4] LC_NUMERIC=C                          
## [5] LC_TIME=English_United States.1252    
## system code page: 65001
## 
## attached base packages:
## [1] stats     graphics  grDevices utils     datasets  methods   base     
## 
## other attached packages:
## [1] reticulate_1.20 tidyr_1.1.3     latex2exp_0.5.0 ggplot2_3.3.5  
## [5] MASS_7.3-54    
## 
## loaded via a namespace (and not attached):
##  [1] Rcpp_1.0.7        bslib_0.2.5.1     compiler_4.1.0    pillar_1.6.2     
##  [5] jquerylib_0.1.4   tools_4.1.0       digest_0.6.27     lattice_0.20-44  
##  [9] jsonlite_1.7.2    evaluate_0.14     lifecycle_1.0.0   tibble_3.1.3     
## [13] gtable_0.3.0      png_0.1-7         pkgconfig_2.0.3   rlang_0.4.11     
## [17] Matrix_1.3-3      yaml_2.2.1        xfun_0.24         withr_2.4.2      
## [21] stringr_1.4.0     dplyr_1.0.7       knitr_1.33        generics_0.1.0   
## [25] fs_1.5.0          sass_0.4.0        vctrs_0.3.8       grid_4.1.0       
## [29] tidyselect_1.1.1  glue_1.4.2        R6_2.5.0          fansi_0.5.0      
## [33] rmarkdown_2.10    purrr_0.3.4       magrittr_2.0.1    scales_1.1.1     
## [37] htmltools_0.5.1.1 ellipsis_0.3.2    colorspace_2.0-2  utf8_1.2.2       
## [41] stringi_1.7.3     munsell_0.5.0     crayon_1.4.1
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{import}\NormalTok{ tensorflow }\ImportTok{as}\NormalTok{ tf }\CommentTok{\# Versão 2.5.0}
\ImportTok{import}\NormalTok{ numpy }\ImportTok{as}\NormalTok{ np      }\CommentTok{\# Versão 1.19.5}
\end{Highlighting}
\end{Shaded}

\hypertarget{a-2}{%
\subsubsection{a)}\label{a-2}}

Faremos a simulação tomando \(\mu_1=[10,10]^t\), \(\mu_2=[0,-10]^t\) e
\(\mu_3=[-10,0]^t\), ademais, usaremos a semente \(13031998\).

\begin{Shaded}
\begin{Highlighting}[]
\DocumentationTok{\#\#\#\# R \#\#\#\#}
\FunctionTok{set.seed}\NormalTok{(}\DecValTok{13031998}\NormalTok{)}
\CommentTok{\# Usando o pacote MASS para a amostragem}
\NormalTok{dataset}\OtherTok{=}\FunctionTok{mvrnorm}\NormalTok{(}\DecValTok{300}\NormalTok{,}\FunctionTok{c}\NormalTok{(}\DecValTok{0}\NormalTok{,}\DecValTok{0}\NormalTok{),}\FunctionTok{diag}\NormalTok{(}\FunctionTok{c}\NormalTok{(}\DecValTok{1}\NormalTok{,}\DecValTok{1}\NormalTok{)))}
\NormalTok{dataset}\OtherTok{=}\NormalTok{dataset}\SpecialCharTok{+}\FunctionTok{matrix}\NormalTok{(}\FunctionTok{c}\NormalTok{(}\FunctionTok{rep}\NormalTok{(}\FunctionTok{c}\NormalTok{(}\DecValTok{10}\NormalTok{,}\DecValTok{10}\NormalTok{),}\DecValTok{100}\NormalTok{),}
                         \FunctionTok{rep}\NormalTok{(}\FunctionTok{c}\NormalTok{(}\DecValTok{0}\NormalTok{,}\SpecialCharTok{{-}}\DecValTok{10}\NormalTok{),}\DecValTok{100}\NormalTok{),}
                         \FunctionTok{rep}\NormalTok{(}\FunctionTok{c}\NormalTok{(}\SpecialCharTok{{-}}\DecValTok{10}\NormalTok{,}\DecValTok{0}\NormalTok{),}\DecValTok{100}\NormalTok{)),}
                       \DecValTok{300}\NormalTok{,}
                       \DecValTok{2}\NormalTok{,}
                       \AttributeTok{byrow =}\NormalTok{ T)}

\NormalTok{dataset}\OtherTok{=}\FunctionTok{as.data.frame}\NormalTok{(}\FunctionTok{cbind}\NormalTok{(dataset,}\FunctionTok{c}\NormalTok{(}\FunctionTok{rep}\NormalTok{(}\DecValTok{1}\NormalTok{,}\DecValTok{100}\NormalTok{),}
                                      \FunctionTok{rep}\NormalTok{(}\DecValTok{2}\NormalTok{,}\DecValTok{100}\NormalTok{),}
                                      \FunctionTok{rep}\NormalTok{(}\DecValTok{3}\NormalTok{,}\DecValTok{100}\NormalTok{))))}
\FunctionTok{names}\NormalTok{(dataset)}\OtherTok{=}\FunctionTok{c}\NormalTok{(}\StringTok{\textquotesingle{}x\_1\textquotesingle{}}\NormalTok{,}\StringTok{\textquotesingle{}x\_2\textquotesingle{}}\NormalTok{,}\StringTok{\textquotesingle{}Grupo\textquotesingle{}}\NormalTok{)}
\NormalTok{dataset}\SpecialCharTok{$}\NormalTok{Grupo}\OtherTok{=}\FunctionTok{as.factor}\NormalTok{(dataset}\SpecialCharTok{$}\NormalTok{Grupo)}
\NormalTok{dataset}\SpecialCharTok{$}\NormalTok{Bias}\OtherTok{=}\DecValTok{1}

\FunctionTok{ggplot}\NormalTok{(dataset)}\SpecialCharTok{+}
  \FunctionTok{geom\_point}\NormalTok{(}\FunctionTok{aes}\NormalTok{(}\AttributeTok{x=}\NormalTok{x\_1,}\AttributeTok{y=}\NormalTok{x\_2,}\AttributeTok{color=}\NormalTok{Grupo))}\SpecialCharTok{+}
  \FunctionTok{scale\_x\_continuous}\NormalTok{(}\FunctionTok{TeX}\NormalTok{(}\StringTok{\textquotesingle{}x\_1\textquotesingle{}}\NormalTok{))}\SpecialCharTok{+}
  \FunctionTok{scale\_y\_continuous}\NormalTok{(}\FunctionTok{TeX}\NormalTok{(}\StringTok{\textquotesingle{}x\_2\textquotesingle{}}\NormalTok{))}\SpecialCharTok{+}
  \FunctionTok{theme\_bw}\NormalTok{()}\SpecialCharTok{+} \FunctionTok{coord\_fixed}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\includegraphics{lista-1_files/figure-latex/unnamed-chunk-1-1.pdf}

\hypertarget{b-2}{%
\subsubsection{b)}\label{b-2}}

Para cada \(x_i\) observado, definiremos as variáveis dummy
\(\bar{1}_i\), \(\bar{2}_i\) e \(\bar{3}_i\) como:

\[
\overline{N}_i=\begin{cases}
1,\text{ se }y_i=N\\
0,\text{ caso contrário.}
\end{cases}
\]

Com isto, temos que, para um vetor de probabilidades
\(\vec{p}_i=[p_{i1},p_{i2},p_{i3}]\):

\[
\mathbb{P}(y_i=k|\vec{p}_i)=p_{ik}^{\bar{k}}
\]

Daí, podemos obter que:

\[
\ln(\mathbb{P}(y_i=k|\vec{p}_i))=\bar{k}\ln (p_{ik})
\]

Vamos supor que \(\vec{p}_i\) pode ser representada pelo seguinte
modelo:

\[p_{ij}=\frac{\exp\{{x_i^t\vec{\beta}_j}\}}{\sum_{l=1}^{3}\exp\{{x_i^t\vec{\beta}_l}\}}\]

Temos que:

\[
\begin{aligned}
\frac{d}{dp_{ij}}\ln(\mathbb{P}(y_i=k|\vec{p}_i))=&\begin{cases}\frac{1}{p_{ik}}, \text{ se }j=k\\ 0,\text{ caso contrário.}\end{cases}\\
\nabla_{\vec{\beta}_{j}}p_{ij}=&\nabla_{\vec{\beta}_{j}}\frac{\exp\{{x_i^t\vec{\beta}_j}\}}{\sum_{l=1}^{3}\exp\{{x_i^t\vec{\beta}_l}\}}=\frac{x_i\exp\{{x_i^t\vec{\beta}_j}\}\sum_{l=1}^{3}\exp\{{x_i^t\vec{\beta}_l}\}-x_i\exp\{{x_i^t\vec{\beta}_j}\}^2}{\left(\sum_{l=1}^{3}\exp\{{x_i^t\vec{\beta}_l}\}\right)^2}\\
=&\frac{x_i\exp\{{x_i^t\vec{\beta}_j}\}\cancel{\sum_{l=1}^{3}\exp\{{x_i^t\vec{\beta}_l}\}}}{\left(\sum_{l=1}^{3}\exp\{{x_i^t\vec{\beta}_l}\}\right)^{\cancel{2}}}-\frac{x_i\exp\{{x_i^t\vec{\beta}_j}\}^2}{\left(\sum_{l=1}^{3}\exp\{{x_i^t\vec{\beta}_l}\}\right)^2}\\
=&x_i\left(\frac{\exp\{{x_i^t\vec{\beta}_j}\}}{\sum_{l=1}^{3}\exp\{{x_i^t\vec{\beta}_l}\}}-\left(\frac{\exp\{{x_i^t\vec{\beta}_j}\}}{\sum_{l=1}^{3}\exp\{x_i^t\vec{\beta}_l\}}\right)^2\right)\\
=&x_i(p_{ij}-p_{ij}^2)=x_ip_{ij}(1-p_{ij})
\end{aligned}
\]

Agora (caso o leitor tenha sobrevivido as contas acima), vamos obter
\(\nabla_{\vec{\beta}_{j}}p_{ik}\) para \(j\neq k\):

\[
\begin{aligned}
\nabla_{\vec{\beta}_{j}}p_{ik}=&\nabla_{\vec{\beta}_{j}}\frac{\exp\{{x_i^t\vec{\beta}_k}\}}{\sum_{l=1}^{3}\exp\{{x_i^t\vec{\beta}_l}\}}=-x_i\frac{\exp\{{x_i^t\vec{\beta}_j}\}\exp\{{x_i^t\vec{\beta}_k}\}}{\left(\sum_{l=1}^{3}\exp\{{x_i^t\vec{\beta}_l}\}\right)^2}\\
=&-x_ip_{ij}p_{ik}
\end{aligned}
\]

Usando a regra da cadeia, obtemos que:

\[
\nabla_{\vec{\beta}_{j}}\ln(\mathbb{P}(y_i=k|x_i,\boldsymbol{\beta})=\begin{cases}
\begin{aligned}
\frac{x_i\cancel{p_{ij}}(1-p_{ij})}{\cancel{p_{ik}}}=x_i(1-p_{ij}),&\text{ se }j=k.\\
\frac{-x_ip_{ij}\cancel{p_{ik}}}{\cancel{p_{ik}}}=-x_ip_{ij},&\text{ se }j\neq k.
\end{aligned}
\end{cases}
\]

A expressão acima pode ser reescrita como:

\[
\nabla_{\vec{\beta}_{j}}\ln(\mathbb{P}(y_i=k|x_i,\boldsymbol{\beta})=x_i(\mathds{1}(y_i=j)-p_{ij})
\]

Agora, ao levarmos em conta todos os dados, obtemos:

\[
\nabla_{\vec{\beta}_{j}}l(\beta_k;\boldsymbol{y},\boldsymbol{x})=\sum_{i=1}^nx_i(\mathds{1}(y_i=j)-p_{ij})
\]

A equação de atualização para os parâmetros do modelo pode ser escrita
como:

\[
\beta_k^{i+1}=\beta_k^{i}+\lambda\nabla_{\vec{\beta}_{j}}l(\beta_k^{i};\boldsymbol{y},\boldsymbol{x})=\beta_k^i+\lambda\sum_{i=1}^nx_i(\mathds{1}(y_i=j)-p_{ij})
\]

Onde \(\beta_k^{i}\) é o valor do vetor \(\beta_k\) na etapa \(i\) do
algoritmo e \(\lambda\) é uma constante arbitrária que regula a
velocidade com a qual os parâmetros são atualizados.

Por último, observe que \(p_{ij}=1-\sum_{k\neq j}p_{ik}\), donde
Concluímos que a equação de atualização do modelo depende da
probabilidade estimada de cada classe.

\hypertarget{c-2}{%
\subsubsection{c)}\label{c-2}}

Trivialmente, escrevendo
\(\vec{\beta}_j+c_i = \vec{\beta}_j+\vec{1}c_i\), onde \(\vec{1}\) é um
vetor de \(1\)'s com dimensão igual a de \(\vec{\beta}_j\), obtemos que:

\[
\begin{aligned}
\mathbb{P}(y_i=j|X,\beta_1+c_i,\beta_2+c_i,\beta_3+c_i)=&\frac{\exp\{{x_i^t(\vec{\beta}_j}+\vec{1}c_i)\}}{\sum_{l=1}^{3}\exp\{{x_i^t(\vec{\beta}_l+\vec{1}c_i)}\}}\\
=&\frac{\exp\{{x_i^t\vec{\beta}_j}+x_i^t\vec{1}c_i\}}{\sum_{l=1}^{3}\exp\{{x_i^t\vec{\beta}_l+x_i^t\vec{1}c_i}\}}\\
=&\frac{\exp\{x_i^t\vec{\beta}_j\}\cancel{\exp\{x_i^t\vec{1}c_i\}}}{\sum_{l=1}^{3}\exp\{{x_i^t(\vec{\beta}_l}\}\cancel{\exp\{x_i^t\vec{1}c_i\}}}\\
=&\frac{\exp\{x_i^t\vec{\beta}_j\}}{\sum_{l=1}^{3}\exp\{{x_i^t(\vec{\beta}_l}\}}\\
=&\mathbb{P}(y_i=j|X,\beta_1,\beta_2,\beta_3)
\end{aligned}
\]

A sentença acima vale para todo \(c_i \in \mathbb{R}\).

\hypertarget{d-1}{%
\subsubsection{d)}\label{d-1}}

Seja \(n\) o número de iterações e \(\lambda\) a taxa de aprendizado,
então o pseudo-código para o algoritmo é:

1 - Inicializemos o algoritmo com \(\beta_j^0=\vec{0}\), para todo
\(j\), \(k=0\) e \(c_i^0=0\).

2 - Computamos
\(p_{ij}=\frac{\exp\{{x_i^t(\vec{\beta}_j^{k}+c_i^k)\}}}{\sum_{l=1}^{3}\exp\{{x_i^t(\vec{\beta}_l^k}+c_i^k)\}}\)
para todo \(i=1,...,n\) e \(j=1,2,3\).

3 - Para cada \(j=2,3\), tomamos:

\[
\beta_j^{k+1}= \beta_j^k+\lambda\sum_{i=1}^nx_i(\mathds{1}(y_i=j)-p_{ij})
\]

4 - Para cada \(i=1,...,n\), tomamos:

\[c_i^{k+1}=-\max_j\{x_i^t\beta_{j}^{k+1}\}\]

5 - Tomamos \(k=k+1\).

6 - Se \(k>n\), encerramos o algoritmo, do contrário, retornamos ao
passo \(2\).

\hypertarget{e}{%
\subsubsection{e)}\label{e}}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\#\#\#\# Python \#\#\#\#}

\KeywordTok{def}\NormalTok{ multinom\_pred\_train(beta,X):}
\NormalTok{  pre\_ativ}\OperatorTok{=}\NormalTok{X }\OperatorTok{@}\NormalTok{ beta}
\NormalTok{  c}\OperatorTok{={-}}\NormalTok{tf.math.reduce\_max(pre\_ativ,axis}\OperatorTok{=}\DecValTok{1}\NormalTok{,keepdims}\OperatorTok{=}\VariableTok{True}\NormalTok{)}
\NormalTok{  ativ}\OperatorTok{=}\NormalTok{tf.math.exp(pre\_ativ}\OperatorTok{+}\NormalTok{c)}
\NormalTok{  ativ}\OperatorTok{=}\NormalTok{ativ}\OperatorTok{/}\NormalTok{(tf.math.reduce\_sum(ativ,axis}\OperatorTok{=}\DecValTok{1}\NormalTok{,keepdims}\OperatorTok{=}\VariableTok{True}\NormalTok{))}
  \ControlFlowTok{return}\NormalTok{ tf.squeeze(ativ)}

\NormalTok{multinom\_pred}\OperatorTok{=}\NormalTok{tf.function(multinom\_pred\_train)}

\AttributeTok{@tf.function}
\KeywordTok{def}\NormalTok{ multinom\_train\_step(beta\_0,X,Y,n\_labels,lamb}\OperatorTok{=}\DecValTok{10}\OperatorTok{**{-}}\DecValTok{3}\NormalTok{):}
\NormalTok{  pred}\OperatorTok{=}\NormalTok{multinom\_pred\_train(beta\_0,X)}
\NormalTok{  pre\_grad}\OperatorTok{=}\NormalTok{ tf.one\_hot(Y,n\_labels)}\OperatorTok{{-}}\NormalTok{pred}
\NormalTok{  grad}\OperatorTok{=}\NormalTok{tf.transpose(X) }\OperatorTok{@}\NormalTok{ pre\_grad}
  \CommentTok{\# Atualizando beta\_0}
  \CommentTok{\# A parte (1{-}tf.one\_hot(tf.zeros(beta\_0.shape[0],dtype=\textquotesingle{}int32\textquotesingle{}),n\_labels))}
  \CommentTok{\# serve para fixar a beta\_1=0.}
\NormalTok{  beta\_0.assign(}
\NormalTok{    beta\_0}\OperatorTok{+}\NormalTok{(}\DecValTok{1}\OperatorTok{{-}}\NormalTok{tf.one\_hot(tf.zeros(beta\_0.shape[}\DecValTok{0}\NormalTok{],dtype}\OperatorTok{=}\StringTok{\textquotesingle{}int32\textquotesingle{}}\NormalTok{),n\_labels))}\OperatorTok{*}\NormalTok{lamb}\OperatorTok{*}\NormalTok{grad}
\NormalTok{    )}

\KeywordTok{def}\NormalTok{ multinom\_train(beta\_0,X,Y,lamb}\OperatorTok{=}\DecValTok{10}\OperatorTok{**{-}}\DecValTok{3}\NormalTok{,n}\OperatorTok{=}\DecValTok{10}\NormalTok{,batch\_size}\OperatorTok{=}\DecValTok{10}\NormalTok{):}
\NormalTok{  erros}\OperatorTok{=}\NormalTok{[]}
\NormalTok{  n\_labels}\OperatorTok{=}\NormalTok{tf.cast(tf.math.reduce\_max(Y)}\OperatorTok{+}\DecValTok{1}\NormalTok{,}\StringTok{\textquotesingle{}int32\textquotesingle{}}\NormalTok{)}
  \ControlFlowTok{for}\NormalTok{ step }\KeywordTok{in} \BuiltInTok{range}\NormalTok{(n):}
    \ControlFlowTok{for}\NormalTok{ i }\KeywordTok{in} \BuiltInTok{range}\NormalTok{(X.shape[}\DecValTok{0}\NormalTok{]}\OperatorTok{//}\NormalTok{batch\_size}\OperatorTok{+}\DecValTok{1}\NormalTok{):}
\NormalTok{      X\_batched}\OperatorTok{=}\NormalTok{X[i}\OperatorTok{*}\NormalTok{batch\_size:(i}\OperatorTok{+}\DecValTok{1}\NormalTok{)}\OperatorTok{*}\NormalTok{batch\_size]}
\NormalTok{      Y\_batched}\OperatorTok{=}\NormalTok{Y[i}\OperatorTok{*}\NormalTok{batch\_size:(i}\OperatorTok{+}\DecValTok{1}\NormalTok{)}\OperatorTok{*}\NormalTok{batch\_size]}
      \ControlFlowTok{if}\NormalTok{ tf.math.reduce\_sum(X\_batched}\OperatorTok{**}\DecValTok{2}\NormalTok{)}\OperatorTok{==}\DecValTok{0}\NormalTok{:}
        \ControlFlowTok{break}
      \ControlFlowTok{if}\NormalTok{ tf.math.reduce\_any(tf.math.logical\_not(tf.math.is\_finite(beta\_0))):}
        \ControlFlowTok{raise} \PreprocessorTok{ValueError}\NormalTok{(}\StringTok{\textquotesingle{}NAN no pesos\textquotesingle{}}\NormalTok{)}
\NormalTok{      multinom\_train\_step(beta\_0,X\_batched,Y\_batched,n\_labels,lamb)}
      
\NormalTok{    pred}\OperatorTok{=}\NormalTok{tf.cast(tf.argmax(multinom\_pred(beta\_0,X),axis}\OperatorTok{=}\DecValTok{1}\NormalTok{),dtype}\OperatorTok{=}\StringTok{\textquotesingle{}int32\textquotesingle{}}\NormalTok{)}
\NormalTok{    score}\OperatorTok{=}\NormalTok{tf.cast(tf.equal(pred,data\_y),dtype}\OperatorTok{=}\StringTok{\textquotesingle{}float32\textquotesingle{}}\NormalTok{)}
\NormalTok{    erros.append(tf.math.reduce\_mean(score).numpy())}
  \ControlFlowTok{return}\NormalTok{ erros}
\end{Highlighting}
\end{Shaded}

\hypertarget{f}{%
\subsubsection{f)}\label{f}}

Para este item, tomamos o tamanho dos pacotes \(m=300\), a taxa de
aprendizado \(\lambda=\frac{1}{m}10^{-10}\) e a quantidade de iterações
\(n=100\). Adiante temos o gráfico da acurácia do modelo ao longo das
iterações. O valor da taxa de aprendizado foi escolhido propositalmente
baixo, de modo que o gráfico ficasse mais interessante (para outros
valores de \(\lambda\) o algoritmo obtem \(100\%\) de precisão na
primeira iteração).

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\#\#\#\# Python \#\#\#\#}

\CommentTok{\# Embaralhando a ordem dos dados}
\NormalTok{index}\OperatorTok{=}\NormalTok{tf.}\BuiltInTok{range}\NormalTok{(}\DecValTok{300}\NormalTok{)}
\NormalTok{tf.random.set\_seed(}\DecValTok{13031998}\NormalTok{)}
\NormalTok{index}\OperatorTok{=}\NormalTok{tf.random.shuffle(index)}
\NormalTok{data\_x}\OperatorTok{=}\NormalTok{tf.gather(tf.constant(np.asarray(r.dataset,dtype}\OperatorTok{=}\StringTok{\textquotesingle{}float32\textquotesingle{}}\NormalTok{)[:,[}\DecValTok{3}\NormalTok{,}\DecValTok{0}\NormalTok{,}\DecValTok{1}\NormalTok{]]),index,axis}\OperatorTok{=}\DecValTok{0}\NormalTok{)}
\NormalTok{data\_y}\OperatorTok{=}\NormalTok{tf.gather(tf.constant(np.asarray(r.dataset,dtype}\OperatorTok{=}\StringTok{\textquotesingle{}int32\textquotesingle{}}\NormalTok{)[:,}\DecValTok{2}\NormalTok{]}\OperatorTok{{-}}\DecValTok{1}\NormalTok{),index,axis}\OperatorTok{=}\DecValTok{0}\NormalTok{)}
\NormalTok{beta}\OperatorTok{=}\NormalTok{tf.Variable(tf.zeros([}\DecValTok{3}\NormalTok{,}\DecValTok{3}\NormalTok{]))}
\NormalTok{n}\OperatorTok{=}\DecValTok{50}
\NormalTok{m}\OperatorTok{=}\DecValTok{300}

\NormalTok{erros}\OperatorTok{=}\NormalTok{multinom\_train(beta,}
\NormalTok{                     data\_x,}
\NormalTok{                     data\_y,}
\NormalTok{                     lamb}\OperatorTok{=}\NormalTok{tf.constant(}\DecValTok{10}\OperatorTok{**{-}}\DecValTok{10}\NormalTok{)}\OperatorTok{/}\NormalTok{m,}
\NormalTok{                     n}\OperatorTok{=}\NormalTok{n,}
\NormalTok{                     batch\_size}\OperatorTok{=}\NormalTok{tf.constant(m,dtype}\OperatorTok{=}\StringTok{\textquotesingle{}int32\textquotesingle{}}\NormalTok{))}
\NormalTok{erros}\OperatorTok{=}\NormalTok{np.asarray(erros)}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\DocumentationTok{\#\#\#\# R \#\#\#\#}
\FunctionTok{ggplot}\NormalTok{()}\SpecialCharTok{+}
  \CommentTok{\#geom\_point(aes(x=c(1:py$n),y=py$erros))+}
  \FunctionTok{geom\_line}\NormalTok{(}\FunctionTok{aes}\NormalTok{(}\AttributeTok{x=}\FunctionTok{c}\NormalTok{(}\DecValTok{1}\SpecialCharTok{:}\NormalTok{py}\SpecialCharTok{$}\NormalTok{n),}\AttributeTok{y=}\NormalTok{py}\SpecialCharTok{$}\NormalTok{erros))}\SpecialCharTok{+}
  \FunctionTok{scale\_x\_continuous}\NormalTok{(}\StringTok{\textquotesingle{}Iteração\textquotesingle{}}\NormalTok{)}\SpecialCharTok{+}
  \FunctionTok{scale\_y\_continuous}\NormalTok{(}\StringTok{\textquotesingle{}Acurácia\textquotesingle{}}\NormalTok{)}\SpecialCharTok{+}
  \FunctionTok{theme\_bw}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\includegraphics{lista-1_files/figure-latex/unnamed-chunk-4-1.pdf}

\hypertarget{questuxe3o-4}{%
\subsection{Questão 4}\label{questuxe3o-4}}

\begin{Shaded}
\begin{Highlighting}[]
\DocumentationTok{\#\#\#\# R \#\#\#\#}
\NormalTok{data\_train}\OtherTok{=}\FunctionTok{read.csv}\NormalTok{(}\StringTok{\textquotesingle{}mnist\_train.csv\textquotesingle{}}\NormalTok{,}\AttributeTok{header=}\NormalTok{F)}
\NormalTok{train\_x}\OtherTok{=}\NormalTok{data\_train[}\DecValTok{1}\SpecialCharTok{:}\DecValTok{55000}\NormalTok{,}\DecValTok{2}\SpecialCharTok{:}\DecValTok{785}\NormalTok{]}
\NormalTok{train\_y}\OtherTok{=}\NormalTok{data\_train[}\DecValTok{1}\SpecialCharTok{:}\DecValTok{55000}\NormalTok{,}\DecValTok{1}\NormalTok{]}

\NormalTok{val\_x}\OtherTok{=}\NormalTok{data\_train[}\DecValTok{55001}\SpecialCharTok{:}\DecValTok{60000}\NormalTok{,}\DecValTok{2}\SpecialCharTok{:}\DecValTok{785}\NormalTok{]}
\NormalTok{val\_y}\OtherTok{=}\NormalTok{data\_train[}\DecValTok{55001}\SpecialCharTok{:}\DecValTok{60000}\NormalTok{,}\DecValTok{1}\NormalTok{]}

\NormalTok{data\_test}\OtherTok{=}\FunctionTok{read.csv}\NormalTok{(}\StringTok{\textquotesingle{}mnist\_test.csv\textquotesingle{}}\NormalTok{,}\AttributeTok{header=}\NormalTok{F)}
\NormalTok{test\_x}\OtherTok{=}\NormalTok{data\_test[,}\DecValTok{2}\SpecialCharTok{:}\DecValTok{785}\NormalTok{]}
\NormalTok{test\_y}\OtherTok{=}\NormalTok{data\_test[,}\DecValTok{1}\NormalTok{]}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\DocumentationTok{\#\#\#\# R \#\#\#\#}
\NormalTok{mat}\OtherTok{=}\FunctionTok{as.data.frame}\NormalTok{(}\FunctionTok{matrix}\NormalTok{(train\_x[}\DecValTok{1}\NormalTok{,],}\DecValTok{28}\NormalTok{,}\DecValTok{28}\NormalTok{))}
\FunctionTok{names}\NormalTok{(mat)}\OtherTok{=}\FunctionTok{c}\NormalTok{(}\DecValTok{1}\SpecialCharTok{:}\DecValTok{28}\NormalTok{)}
\NormalTok{mat}\SpecialCharTok{$}\NormalTok{Row}\OtherTok{=}\FunctionTok{c}\NormalTok{(}\DecValTok{1}\SpecialCharTok{:}\DecValTok{28}\NormalTok{)}
\NormalTok{mat}\SpecialCharTok{$}\NormalTok{Index}\OtherTok{=}\DecValTok{1}
\NormalTok{data}\OtherTok{=}\FunctionTok{as.data.frame}\NormalTok{(}\FunctionTok{pivot\_longer}\NormalTok{(mat,}\FunctionTok{c}\NormalTok{(}\DecValTok{1}\SpecialCharTok{:}\DecValTok{28}\NormalTok{)))}
\ControlFlowTok{for}\NormalTok{(i }\ControlFlowTok{in} \FunctionTok{c}\NormalTok{(}\DecValTok{2}\SpecialCharTok{:}\DecValTok{5}\NormalTok{))\{}
\NormalTok{  mat}\OtherTok{=}\FunctionTok{as.data.frame}\NormalTok{(}\FunctionTok{matrix}\NormalTok{(train\_x[i,],}\DecValTok{28}\NormalTok{,}\DecValTok{28}\NormalTok{))}
  \FunctionTok{names}\NormalTok{(mat)}\OtherTok{=}\FunctionTok{c}\NormalTok{(}\DecValTok{28}\SpecialCharTok{:}\DecValTok{1}\NormalTok{)}
\NormalTok{  mat}\SpecialCharTok{$}\NormalTok{Row}\OtherTok{=}\FunctionTok{c}\NormalTok{(}\DecValTok{1}\SpecialCharTok{:}\DecValTok{28}\NormalTok{)}
\NormalTok{  mat}\SpecialCharTok{$}\NormalTok{Index}\OtherTok{=}\NormalTok{i}
\NormalTok{  data}\OtherTok{=}\FunctionTok{rbind}\NormalTok{(data,}\FunctionTok{as.data.frame}\NormalTok{(}\FunctionTok{pivot\_longer}\NormalTok{(mat,}\FunctionTok{c}\NormalTok{(}\DecValTok{1}\SpecialCharTok{:}\DecValTok{28}\NormalTok{))))}
\NormalTok{\}}

\NormalTok{data}\SpecialCharTok{$}\NormalTok{name}\OtherTok{=}\FunctionTok{as.numeric}\NormalTok{(data}\SpecialCharTok{$}\NormalTok{name)}
\NormalTok{data}\SpecialCharTok{$}\NormalTok{value}\OtherTok{=}\FunctionTok{as.numeric}\NormalTok{(data}\SpecialCharTok{$}\NormalTok{value)}
\NormalTok{data}\SpecialCharTok{$}\NormalTok{Index}\OtherTok{=}\FunctionTok{as.factor}\NormalTok{(data}\SpecialCharTok{$}\NormalTok{Index)}
\FunctionTok{ggplot}\NormalTok{(data)}\SpecialCharTok{+}
  \FunctionTok{geom\_tile}\NormalTok{(}\FunctionTok{aes}\NormalTok{(}\AttributeTok{x=}\NormalTok{Row,}\AttributeTok{y=}\NormalTok{name,}\AttributeTok{fill=}\NormalTok{value))}\SpecialCharTok{+}
  \FunctionTok{scale\_fill\_gradient}\NormalTok{(}\AttributeTok{high=}\StringTok{\textquotesingle{}\#000000\textquotesingle{}}\NormalTok{,}\AttributeTok{low=}\StringTok{\textquotesingle{}\#ffffff\textquotesingle{}}\NormalTok{)}\SpecialCharTok{+}
  \FunctionTok{guides}\NormalTok{(}\AttributeTok{fill=}\StringTok{\textquotesingle{}none\textquotesingle{}}\NormalTok{)}\SpecialCharTok{+}
  \FunctionTok{facet\_wrap}\NormalTok{(}\SpecialCharTok{\textasciitilde{}}\NormalTok{Index)}\SpecialCharTok{+}
  \FunctionTok{theme\_void}\NormalTok{()}\SpecialCharTok{+}
  \FunctionTok{theme}\NormalTok{(}\AttributeTok{strip.background =} \FunctionTok{element\_blank}\NormalTok{(),}
        \AttributeTok{strip.text.x =} \FunctionTok{element\_blank}\NormalTok{())}
\end{Highlighting}
\end{Shaded}

\includegraphics{lista-1_files/figure-latex/unnamed-chunk-6-1.pdf}

\hypertarget{b-3}{%
\subsubsection{b)}\label{b-3}}

\begin{Shaded}
\begin{Highlighting}[]
\DocumentationTok{\#\#\#\# R \#\#\#\#}
\NormalTok{data}\OtherTok{=}\FunctionTok{data.frame}\NormalTok{(}\AttributeTok{index=}\FunctionTok{c}\NormalTok{(}\DecValTok{1}\SpecialCharTok{:}\DecValTok{784}\NormalTok{),}\AttributeTok{values=}\FunctionTok{as.vector}\NormalTok{(}\FunctionTok{t}\NormalTok{(train\_x[}\DecValTok{1}\NormalTok{,])))}
\FunctionTok{ggplot}\NormalTok{(data,}\FunctionTok{aes}\NormalTok{(values))}\SpecialCharTok{+}
  \FunctionTok{geom\_histogram}\NormalTok{(}\AttributeTok{bins=}\DecValTok{10}\NormalTok{)}\SpecialCharTok{+}
  \FunctionTok{scale\_x\_continuous}\NormalTok{(}\StringTok{\textquotesingle{}Intensidade dos pixels\textquotesingle{}}\NormalTok{)}\SpecialCharTok{+}
  \FunctionTok{scale\_y\_continuous}\NormalTok{(}\StringTok{\textquotesingle{}Contagem\textquotesingle{}}\NormalTok{,}\AttributeTok{expand=}\FunctionTok{c}\NormalTok{(}\DecValTok{0}\NormalTok{,}\DecValTok{0}\NormalTok{,}\FloatTok{0.1}\NormalTok{,}\DecValTok{0}\NormalTok{))}\SpecialCharTok{+}
  \FunctionTok{theme\_bw}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\includegraphics{lista-1_files/figure-latex/unnamed-chunk-7-1.pdf}

Com base no histograma acima e observando que a intensidade dos pixels
varia no intervalo \([0,255]\), proponho dividir a intensidade dos
pixels por 255. Segue o histograma depois da transformação proposta:

\begin{Shaded}
\begin{Highlighting}[]
\DocumentationTok{\#\#\#\# R \#\#\#\#}
\NormalTok{data}\OtherTok{=}\FunctionTok{data.frame}\NormalTok{(}\AttributeTok{index=}\FunctionTok{c}\NormalTok{(}\DecValTok{1}\SpecialCharTok{:}\DecValTok{784}\NormalTok{),}\AttributeTok{values=}\FunctionTok{as.vector}\NormalTok{(}\FunctionTok{t}\NormalTok{(train\_x[}\DecValTok{1}\NormalTok{,])))}
\FunctionTok{ggplot}\NormalTok{(data,}\FunctionTok{aes}\NormalTok{(values}\SpecialCharTok{/}\DecValTok{255}\NormalTok{))}\SpecialCharTok{+}
  \FunctionTok{geom\_histogram}\NormalTok{(}\AttributeTok{bins=}\DecValTok{10}\NormalTok{)}\SpecialCharTok{+}
  \FunctionTok{scale\_x\_continuous}\NormalTok{(}\StringTok{\textquotesingle{}Intensidade dos pixels\textquotesingle{}}\NormalTok{)}\SpecialCharTok{+}
  \FunctionTok{scale\_y\_continuous}\NormalTok{(}\StringTok{\textquotesingle{}Contagem\textquotesingle{}}\NormalTok{,}\AttributeTok{expand=}\FunctionTok{c}\NormalTok{(}\DecValTok{0}\NormalTok{,}\DecValTok{0}\NormalTok{,}\FloatTok{0.1}\NormalTok{,}\DecValTok{0}\NormalTok{))}\SpecialCharTok{+}
  \FunctionTok{theme\_bw}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\includegraphics{lista-1_files/figure-latex/unnamed-chunk-8-1.pdf}

\hypertarget{c-3}{%
\subsubsection{c)}\label{c-3}}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\#\#\#\# Python \#\#\#\#}
\KeywordTok{def}\NormalTok{ multinom\_train(beta\_0,X,Y,X\_val}\OperatorTok{=}\VariableTok{None}\NormalTok{,Y\_val}\OperatorTok{=}\VariableTok{None}\NormalTok{,lamb}\OperatorTok{=}\DecValTok{10}\OperatorTok{**{-}}\DecValTok{3}\NormalTok{,n}\OperatorTok{=}\DecValTok{10}\NormalTok{,batch\_size}\OperatorTok{=}\DecValTok{10}\NormalTok{):}
\NormalTok{  erros}\OperatorTok{=}\NormalTok{[]}
\NormalTok{  n\_labels}\OperatorTok{=}\NormalTok{tf.cast(tf.math.reduce\_max(Y)}\OperatorTok{+}\DecValTok{1}\NormalTok{,}\StringTok{\textquotesingle{}int32\textquotesingle{}}\NormalTok{)}
  \ControlFlowTok{for}\NormalTok{ step }\KeywordTok{in} \BuiltInTok{range}\NormalTok{(n):}
    \ControlFlowTok{for}\NormalTok{ i }\KeywordTok{in} \BuiltInTok{range}\NormalTok{(X.shape[}\DecValTok{0}\NormalTok{]}\OperatorTok{//}\NormalTok{batch\_size}\OperatorTok{+}\DecValTok{1}\NormalTok{):}
\NormalTok{      X\_batched}\OperatorTok{=}\NormalTok{X[i}\OperatorTok{*}\NormalTok{batch\_size:(i}\OperatorTok{+}\DecValTok{1}\NormalTok{)}\OperatorTok{*}\NormalTok{batch\_size]}
\NormalTok{      Y\_batched}\OperatorTok{=}\NormalTok{Y[i}\OperatorTok{*}\NormalTok{batch\_size:(i}\OperatorTok{+}\DecValTok{1}\NormalTok{)}\OperatorTok{*}\NormalTok{batch\_size]}
      \ControlFlowTok{if}\NormalTok{ tf.math.reduce\_sum(X\_batched}\OperatorTok{**}\DecValTok{2}\NormalTok{)}\OperatorTok{==}\DecValTok{0}\NormalTok{:}
        \ControlFlowTok{break}
      \ControlFlowTok{if}\NormalTok{ tf.math.reduce\_any(tf.math.logical\_not(tf.math.is\_finite(beta\_0))):}
        \ControlFlowTok{raise} \PreprocessorTok{ValueError}\NormalTok{(}\StringTok{\textquotesingle{}NAN no pesos\textquotesingle{}}\NormalTok{)}
\NormalTok{      multinom\_train\_step(beta\_0,X\_batched,Y\_batched,n\_labels,lamb)}
    \ControlFlowTok{if}\NormalTok{ X\_val }\KeywordTok{is} \KeywordTok{not} \VariableTok{None} \KeywordTok{and}\NormalTok{ Y\_val }\KeywordTok{is} \KeywordTok{not} \VariableTok{None}\NormalTok{:}
\NormalTok{      pred}\OperatorTok{=}\NormalTok{tf.cast(tf.argmax(multinom\_pred(beta\_0,X\_val),axis}\OperatorTok{=}\DecValTok{1}\NormalTok{),dtype}\OperatorTok{=}\StringTok{\textquotesingle{}int32\textquotesingle{}}\NormalTok{)}
\NormalTok{      score}\OperatorTok{=}\NormalTok{tf.cast(tf.equal(pred,Y\_val),dtype}\OperatorTok{=}\StringTok{\textquotesingle{}float32\textquotesingle{}}\NormalTok{)}
\NormalTok{      score}\OperatorTok{=}\NormalTok{tf.math.reduce\_mean(score)}
\NormalTok{      erros.append(score)}
\NormalTok{      tf.}\BuiltInTok{print}\NormalTok{(score)}
    
  \ControlFlowTok{return}\NormalTok{ erros}
\end{Highlighting}
\end{Shaded}

\hypertarget{d-2}{%
\subsubsection{d)}\label{d-2}}

Observamos que o tamanho dos batch's estava causando instabilidade, pois
a equação de atualização é um somatório nos elementos da amostra, assim,
para batch's muito grandes, tínhamos um gradiente igualmente grande em
módulo, dificultando a convergência do algoritmo. Para fazer com que a
velocidade de treino não dependesse do tamanho do pacote, tomamos como
taxa de aprendizado \(\lambda=\frac{1}{m}\gamma\), onde \(\gamma\) é a
taxa de aprendizado proposta inicialmente e \(m\) é a quantidade de
elementos em cada batch.

Com a modificações acima e tomando o tamanho dos pacotes \(m=100\),
obtemos o seguinte resultado.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\#\#\#\# Python \#\#\#\#}

\CommentTok{\# Adicionando coluna de bias aos dados.}
\NormalTok{data\_x}\OperatorTok{=}\NormalTok{tf.concat([tf.ones([}\DecValTok{55000}\NormalTok{,}\DecValTok{1}\NormalTok{],dtype}\OperatorTok{=}\StringTok{\textquotesingle{}float32\textquotesingle{}}\NormalTok{),}
\NormalTok{                  tf.constant(r.train\_x,dtype}\OperatorTok{=}\StringTok{\textquotesingle{}float32\textquotesingle{}}\NormalTok{)}\OperatorTok{/}\DecValTok{255}\NormalTok{],}
\NormalTok{                axis}\OperatorTok{=}\DecValTok{1}\NormalTok{)}
\NormalTok{val\_x}\OperatorTok{=}\NormalTok{tf.concat([tf.ones([}\DecValTok{5000}\NormalTok{,}\DecValTok{1}\NormalTok{],dtype}\OperatorTok{=}\StringTok{\textquotesingle{}float32\textquotesingle{}}\NormalTok{),}
\NormalTok{                 tf.constant(r.val\_x,dtype}\OperatorTok{=}\StringTok{\textquotesingle{}float32\textquotesingle{}}\NormalTok{)}\OperatorTok{/}\DecValTok{255}\NormalTok{],}
\NormalTok{                axis}\OperatorTok{=}\DecValTok{1}\NormalTok{)}
\NormalTok{test\_x}\OperatorTok{=}\NormalTok{tf.concat([tf.ones([}\DecValTok{10000}\NormalTok{,}\DecValTok{1}\NormalTok{],dtype}\OperatorTok{=}\StringTok{\textquotesingle{}float32\textquotesingle{}}\NormalTok{),}
\NormalTok{                  tf.constant(r.test\_x,dtype}\OperatorTok{=}\StringTok{\textquotesingle{}float32\textquotesingle{}}\NormalTok{)}\OperatorTok{/}\DecValTok{255}\NormalTok{],}
\NormalTok{                axis}\OperatorTok{=}\DecValTok{1}\NormalTok{)}
              
\CommentTok{\# Salvando conjunto de validação}
\NormalTok{val\_y}\OperatorTok{=}\NormalTok{tf.constant(r.val\_y)}
\NormalTok{data\_y}\OperatorTok{=}\NormalTok{tf.constant(r.train\_y)}
\NormalTok{test\_y}\OperatorTok{=}\NormalTok{tf.constant(r.test\_y)}

\NormalTok{batch\_size}\OperatorTok{=}\DecValTok{500}
\NormalTok{n}\OperatorTok{=}\DecValTok{200}

\CommentTok{\# Treino para gamma=2}
\NormalTok{beta}\OperatorTok{=}\NormalTok{tf.Variable(tf.zeros([}\DecValTok{785}\NormalTok{,}\DecValTok{10}\NormalTok{]))}
\NormalTok{erros\_l1}\OperatorTok{=}\NormalTok{multinom\_train(beta,}
\NormalTok{                     data\_x,}
\NormalTok{                     data\_y,}
\NormalTok{                     val\_x,}
\NormalTok{                     val\_y,}
\NormalTok{                     lamb}\OperatorTok{=}\NormalTok{tf.constant((}\DecValTok{2}\OperatorTok{*}\DecValTok{10}\OperatorTok{**{-}}\DecValTok{0}\NormalTok{)}\OperatorTok{/}\NormalTok{batch\_size,dtype}\OperatorTok{=}\StringTok{\textquotesingle{}float32\textquotesingle{}}\NormalTok{),}
\NormalTok{                     n}\OperatorTok{=}\NormalTok{n,}
\NormalTok{                     batch\_size}\OperatorTok{=}\NormalTok{tf.constant(batch\_size,dtype}\OperatorTok{=}\StringTok{\textquotesingle{}int32\textquotesingle{}}\NormalTok{))}
                     
\CommentTok{\# Treino para gamma=1}
\NormalTok{beta}\OperatorTok{=}\NormalTok{tf.Variable(tf.zeros([}\DecValTok{785}\NormalTok{,}\DecValTok{10}\NormalTok{]))}
\NormalTok{erros\_l2}\OperatorTok{=}\NormalTok{multinom\_train(beta,}
\NormalTok{                     data\_x,}
\NormalTok{                     data\_y,}
\NormalTok{                     val\_x,}
\NormalTok{                     val\_y,}
\NormalTok{                     lamb}\OperatorTok{=}\NormalTok{tf.constant((}\DecValTok{10}\OperatorTok{**{-}}\DecValTok{0}\NormalTok{)}\OperatorTok{/}\NormalTok{batch\_size,dtype}\OperatorTok{=}\StringTok{\textquotesingle{}float32\textquotesingle{}}\NormalTok{),}
\NormalTok{                     n}\OperatorTok{=}\NormalTok{n,}
\NormalTok{                     batch\_size}\OperatorTok{=}\NormalTok{tf.constant(batch\_size,dtype}\OperatorTok{=}\StringTok{\textquotesingle{}int32\textquotesingle{}}\NormalTok{))}

\CommentTok{\# Treino para gamma=0.1                     }
\NormalTok{beta}\OperatorTok{=}\NormalTok{tf.Variable(tf.zeros([}\DecValTok{785}\NormalTok{,}\DecValTok{10}\NormalTok{]))}
\NormalTok{erros\_l3}\OperatorTok{=}\NormalTok{multinom\_train(beta,}
\NormalTok{                     data\_x,}
\NormalTok{                     data\_y,}
\NormalTok{                     val\_x,}
\NormalTok{                     val\_y,}
\NormalTok{                     lamb}\OperatorTok{=}\NormalTok{tf.constant((}\DecValTok{10}\OperatorTok{**{-}}\DecValTok{1}\NormalTok{)}\OperatorTok{/}\NormalTok{batch\_size,dtype}\OperatorTok{=}\StringTok{\textquotesingle{}float32\textquotesingle{}}\NormalTok{),}
\NormalTok{                     n}\OperatorTok{=}\NormalTok{n,}
\NormalTok{                     batch\_size}\OperatorTok{=}\NormalTok{tf.constant(batch\_size,dtype}\OperatorTok{=}\StringTok{\textquotesingle{}int32\textquotesingle{}}\NormalTok{))}

\CommentTok{\# Treino para gamma=0.01                }
\NormalTok{beta}\OperatorTok{=}\NormalTok{tf.Variable(tf.zeros([}\DecValTok{785}\NormalTok{,}\DecValTok{10}\NormalTok{]))}
\NormalTok{erros\_l4}\OperatorTok{=}\NormalTok{multinom\_train(beta,}
\NormalTok{                     data\_x,}
\NormalTok{                     data\_y,}
\NormalTok{                     val\_x,}
\NormalTok{                     val\_y,}
\NormalTok{                     lamb}\OperatorTok{=}\NormalTok{tf.constant((}\DecValTok{10}\OperatorTok{**{-}}\DecValTok{2}\NormalTok{)}\OperatorTok{/}\NormalTok{batch\_size,dtype}\OperatorTok{=}\StringTok{\textquotesingle{}float32\textquotesingle{}}\NormalTok{),}
\NormalTok{                     n}\OperatorTok{=}\NormalTok{n,}
\NormalTok{                     batch\_size}\OperatorTok{=}\NormalTok{tf.constant(batch\_size,dtype}\OperatorTok{=}\StringTok{\textquotesingle{}int32\textquotesingle{}}\NormalTok{))}
                     
\CommentTok{\# Criando matriz com as acurácias ao longo do treino}
\NormalTok{erros}\OperatorTok{=}\NormalTok{np.asarray([erros\_l1,erros\_l2,erros\_l3,erros\_l4])}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\DocumentationTok{\#\#\#\# R \#\#\#\#}
\NormalTok{indices}\OtherTok{=}\FunctionTok{c}\NormalTok{(}\DecValTok{1}\SpecialCharTok{:}\NormalTok{py}\SpecialCharTok{$}\NormalTok{n)}
\NormalTok{precisao}\OtherTok{=}\FunctionTok{cbind}\NormalTok{(indices,}\FunctionTok{as.data.frame}\NormalTok{(}\FunctionTok{t}\NormalTok{(py}\SpecialCharTok{$}\NormalTok{erros)))}
\FunctionTok{names}\NormalTok{(precisao)}\OtherTok{=}\FunctionTok{c}\NormalTok{(}\StringTok{\textquotesingle{}Tempo\textquotesingle{}}\NormalTok{,}\StringTok{\textquotesingle{}lambda\_1\textquotesingle{}}\NormalTok{,}\StringTok{\textquotesingle{}lambda\_2\textquotesingle{}}\NormalTok{,}\StringTok{\textquotesingle{}lambda\_3\textquotesingle{}}\NormalTok{,}\StringTok{\textquotesingle{}lambda\_4\textquotesingle{}}\NormalTok{)}
\NormalTok{precisao}\OtherTok{=}\FunctionTok{as.data.frame}\NormalTok{(}\FunctionTok{pivot\_longer}\NormalTok{(precisao,}\FunctionTok{c}\NormalTok{(}\DecValTok{2}\SpecialCharTok{:}\DecValTok{5}\NormalTok{)))}
\FunctionTok{names}\NormalTok{(precisao)}\OtherTok{=}\FunctionTok{c}\NormalTok{(}\StringTok{\textquotesingle{}Iteração\textquotesingle{}}\NormalTok{,}\StringTok{\textquotesingle{}Taxa\_de\_aprendizado\textquotesingle{}}\NormalTok{,}\StringTok{\textquotesingle{}Acuracia\textquotesingle{}}\NormalTok{)}
\NormalTok{precisao}\SpecialCharTok{$}\NormalTok{Taxa\_de\_aprendizado}\OtherTok{=}\FunctionTok{as.factor}\NormalTok{(precisao}\SpecialCharTok{$}\NormalTok{Taxa\_de\_aprendizado)}

\FunctionTok{ggplot}\NormalTok{(precisao)}\SpecialCharTok{+}
  \CommentTok{\#geom\_point(aes(x=Iteração,y=Acuracia,color=Taxa\_de\_aprendizado))+}
  \FunctionTok{geom\_line}\NormalTok{(}\FunctionTok{aes}\NormalTok{(}\AttributeTok{x=}\NormalTok{Iteração,}\AttributeTok{y=}\NormalTok{Acuracia,}\AttributeTok{color=}\NormalTok{Taxa\_de\_aprendizado))}\SpecialCharTok{+}
  \FunctionTok{scale\_color\_hue}\NormalTok{(}\StringTok{\textquotesingle{}Taxa de aprendizado\textquotesingle{}}\NormalTok{,}
                  \AttributeTok{labels=}\FunctionTok{c}\NormalTok{(}\StringTok{\textquotesingle{}lambda\_1\textquotesingle{}}\OtherTok{=}\FunctionTok{parse}\NormalTok{(}\AttributeTok{text =} \FunctionTok{TeX}\NormalTok{(}\StringTok{\textquotesingle{}$}\SpecialCharTok{\textbackslash{}\textbackslash{}}\StringTok{gamma==2$\textquotesingle{}}\NormalTok{)),}
                           \StringTok{\textquotesingle{}lambda\_2\textquotesingle{}}\OtherTok{=}\FunctionTok{parse}\NormalTok{(}\AttributeTok{text =} \FunctionTok{TeX}\NormalTok{(}\StringTok{\textquotesingle{}$}\SpecialCharTok{\textbackslash{}\textbackslash{}}\StringTok{gamma==1$\textquotesingle{}}\NormalTok{)),}
                           \StringTok{\textquotesingle{}lambda\_3\textquotesingle{}}\OtherTok{=}\FunctionTok{parse}\NormalTok{(}\AttributeTok{text =} \FunctionTok{TeX}\NormalTok{(}\StringTok{\textquotesingle{}$}\SpecialCharTok{\textbackslash{}\textbackslash{}}\StringTok{gamma==0.1$\textquotesingle{}}\NormalTok{)),}
                           \StringTok{\textquotesingle{}lambda\_4\textquotesingle{}}\OtherTok{=}\FunctionTok{parse}\NormalTok{(}\AttributeTok{text =} \FunctionTok{TeX}\NormalTok{(}\StringTok{\textquotesingle{}$}\SpecialCharTok{\textbackslash{}\textbackslash{}}\StringTok{gamma==0.01$\textquotesingle{}}\NormalTok{))))}\SpecialCharTok{+}
  \FunctionTok{scale\_x\_continuous}\NormalTok{(}\StringTok{\textquotesingle{}Iteração\textquotesingle{}}\NormalTok{)}\SpecialCharTok{+}
  \FunctionTok{scale\_y\_continuous}\NormalTok{(}\StringTok{\textquotesingle{}Acurácia\textquotesingle{}}\NormalTok{)}\SpecialCharTok{+}
  \FunctionTok{theme\_bw}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\includegraphics{lista-1_files/figure-latex/unnamed-chunk-11-1.pdf}

Podemos observar que a taxa de aprendizado \(\gamma=0.1\) trouxe o
melhor resultado.

\hypertarget{e-1}{%
\subsubsection{e)}\label{e-1}}

Tomando a taxa de aprendizado \(\gamma=0.1\), obtermos uma precisão de
\(92.66\%\) no conjunto de testes, que um valor significativamente menor
do que o que foi obtido no item anterior (\(94.26\%\)) com a mesma
configuração de treino. Isto pode indicar que há overfitting através dos
hiper-parâmetros, porém, mais estudos seriam necessários para chegar a
uma conclusão. Adiante, temos o código usado para treino e o gráfico da
evolução da acurácia ao longo do treino:

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\#\#\#\# Python \#\#\#\#}

\CommentTok{\# Adicionando coluna de bias aos dados.}
\NormalTok{data\_x}\OperatorTok{=}\NormalTok{tf.concat([tf.ones([}\DecValTok{55000}\NormalTok{,}\DecValTok{1}\NormalTok{],dtype}\OperatorTok{=}\StringTok{\textquotesingle{}float32\textquotesingle{}}\NormalTok{),}
\NormalTok{                  tf.constant(r.train\_x,dtype}\OperatorTok{=}\StringTok{\textquotesingle{}float32\textquotesingle{}}\NormalTok{)}\OperatorTok{/}\DecValTok{255}\NormalTok{],}
\NormalTok{                axis}\OperatorTok{=}\DecValTok{1}\NormalTok{)}
\NormalTok{val\_x}\OperatorTok{=}\NormalTok{tf.concat([tf.ones([}\DecValTok{5000}\NormalTok{,}\DecValTok{1}\NormalTok{],dtype}\OperatorTok{=}\StringTok{\textquotesingle{}float32\textquotesingle{}}\NormalTok{),}
\NormalTok{                 tf.constant(r.val\_x,dtype}\OperatorTok{=}\StringTok{\textquotesingle{}float32\textquotesingle{}}\NormalTok{)}\OperatorTok{/}\DecValTok{255}\NormalTok{],}
\NormalTok{                axis}\OperatorTok{=}\DecValTok{1}\NormalTok{)}
\NormalTok{data\_x}\OperatorTok{=}\NormalTok{tf.concat([data\_x,val\_x],axis}\OperatorTok{=}\DecValTok{0}\NormalTok{)}
\NormalTok{test\_x}\OperatorTok{=}\NormalTok{tf.concat([tf.ones([}\DecValTok{10000}\NormalTok{,}\DecValTok{1}\NormalTok{],dtype}\OperatorTok{=}\StringTok{\textquotesingle{}float32\textquotesingle{}}\NormalTok{),}
\NormalTok{                  tf.constant(r.test\_x,dtype}\OperatorTok{=}\StringTok{\textquotesingle{}float32\textquotesingle{}}\NormalTok{)}\OperatorTok{/}\DecValTok{255}\NormalTok{],}
\NormalTok{                axis}\OperatorTok{=}\DecValTok{1}\NormalTok{)}
              
\CommentTok{\# Salvando conjunto de validação}
\NormalTok{data\_y}\OperatorTok{=}\NormalTok{tf.constant(r.train\_y)}
\NormalTok{val\_y}\OperatorTok{=}\NormalTok{tf.constant(r.val\_y)}
\NormalTok{data\_y}\OperatorTok{=}\NormalTok{tf.concat([data\_y,val\_y],axis}\OperatorTok{=}\DecValTok{0}\NormalTok{)}
\NormalTok{test\_y}\OperatorTok{=}\NormalTok{tf.constant(r.test\_y)}

\NormalTok{batch\_size}\OperatorTok{=}\DecValTok{500}
\NormalTok{n}\OperatorTok{=}\DecValTok{500}

\CommentTok{\# Treino para gamma=0.1}
\NormalTok{beta}\OperatorTok{=}\NormalTok{tf.Variable(tf.zeros([}\DecValTok{785}\NormalTok{,}\DecValTok{10}\NormalTok{]))}
\NormalTok{erros}\OperatorTok{=}\NormalTok{multinom\_train(beta,}
\NormalTok{                     data\_x,}
\NormalTok{                     data\_y,}
\NormalTok{                     test\_x,}
\NormalTok{                     test\_y,}
\NormalTok{                     lamb}\OperatorTok{=}\NormalTok{tf.constant((}\DecValTok{10}\OperatorTok{**{-}}\DecValTok{1}\NormalTok{)}\OperatorTok{/}\NormalTok{batch\_size,dtype}\OperatorTok{=}\StringTok{\textquotesingle{}float32\textquotesingle{}}\NormalTok{),}
\NormalTok{                     n}\OperatorTok{=}\NormalTok{n,}
\NormalTok{                     batch\_size}\OperatorTok{=}\NormalTok{tf.constant(batch\_size,dtype}\OperatorTok{=}\StringTok{\textquotesingle{}int32\textquotesingle{}}\NormalTok{))}

\CommentTok{\# Criando matriz com as acurácias ao longo do treino}
\NormalTok{erros}\OperatorTok{=}\NormalTok{np.asarray(erros)}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\DocumentationTok{\#\#\#\# R \#\#\#\#}
\FunctionTok{ggplot}\NormalTok{()}\SpecialCharTok{+}
  \CommentTok{\#geom\_point(aes(x=c(1:py$n),y=py$erros))+}
  \FunctionTok{geom\_line}\NormalTok{(}\FunctionTok{aes}\NormalTok{(}\AttributeTok{x=}\FunctionTok{c}\NormalTok{(}\DecValTok{1}\SpecialCharTok{:}\NormalTok{py}\SpecialCharTok{$}\NormalTok{n),}\AttributeTok{y=}\NormalTok{py}\SpecialCharTok{$}\NormalTok{erros))}\SpecialCharTok{+}
  \FunctionTok{scale\_x\_continuous}\NormalTok{(}\StringTok{\textquotesingle{}Iteração\textquotesingle{}}\NormalTok{)}\SpecialCharTok{+}
  \FunctionTok{scale\_y\_continuous}\NormalTok{(}\StringTok{\textquotesingle{}Acurácia\textquotesingle{}}\NormalTok{)}\SpecialCharTok{+}
  \FunctionTok{theme\_bw}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\includegraphics{lista-1_files/figure-latex/unnamed-chunk-13-1.pdf}

\end{document}
